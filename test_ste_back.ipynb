{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 14:49:11.681208: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-17 14:49:11.692861: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742237351.707157 1646869 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742237351.711830 1646869 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-17 14:49:11.728128: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from quantization import *\n",
    "from train_quantization import init_seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------before--------\n",
      "name: full_precision_weight, param: tensor([[-0.0037,  0.2682, -0.4115, -0.3680]])\n",
      "name: alpha_w, param: 0.156248539686203\n",
      "name: beta_w, param: -0.1287534236907959\n",
      "name: alpha_x, param: 0.3171935975551605\n",
      "name: beta_x, param: 0.419644832611084\n",
      "name: module.weight, param: tensor([[-0.0037,  0.2682, -0.4115, -0.3680]])\n",
      "name: module.bias, param: tensor([-0.1926])\n",
      "--------after--------\n",
      "y: tensor([[-5150.3291]], grad_fn=<AddmmBackward0>)\n",
      "name: full_precision_weight, param: tensor([[-10000.0039,  -9999.7314, -10000.4111, -10000.3682]])\n",
      "name: alpha_w, param: 0.155760258436203\n",
      "name: beta_w, param: -0.1287534236907959\n",
      "name: alpha_x, param: 0.3171935975551605\n",
      "name: beta_x, param: 0.419644832611084\n",
      "name: module.weight, param: tensor([[-10000.0039,  -9999.7314, -10000.4111, -10000.3682]])\n",
      "name: module.bias, param: tensor([-1.1926])\n",
      "name: full_precision_weight, param: tensor([[10000., 10000., 10000., 10000.]])\n",
      "name: alpha_w, param: 0.00048828125\n",
      "name: beta_w, param: 0.0\n",
      "name: alpha_x, param: None\n",
      "name: beta_x, param: None\n",
      "name: module.weight, param: None\n",
      "name: module.bias, param: tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "init_seeds(0)\n",
    "\n",
    "model = nn.Linear(4, 1)\n",
    "qt_model = QuantizedWrapper(model, quant_w=True, quant_x=True, lsq=1, \n",
    "                            bitwidth_w=4, intbit_w=0, bitwidth_x=4, intbit_x=0, alpha_init=2)\n",
    "qt_model.g_w = 1.\n",
    "optimizer = optim.SGD(qt_model.parameters(), lr = 1)\n",
    "\n",
    "print('--------before--------')\n",
    "for name, param in qt_model.named_parameters():\n",
    "    # if name == 'alpha_x' or name == 'alpha_w':\n",
    "    print(f'name: {name}, param: {param.data}')\n",
    "\n",
    "qt_model.zero_grad()\n",
    "x = torch.tensor((10000,10000,10000,10000)).view(1, 4)\n",
    "y = qt_model(x)\n",
    "y.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print('--------after--------')\n",
    "print(f'y: {y}')\n",
    "\n",
    "for name, param in qt_model.named_parameters():\n",
    "    print(f'name: {name}, param: {param.data}')\n",
    "\n",
    "for name, param in qt_model.named_parameters():\n",
    "    # if name == 'alpha_x' or name == 'alpha_w':\n",
    "    print(f'name: {name}, param: {param.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------before--------\n",
      "name: full_precision_weight, param: tensor([[-0.0037,  0.2682, -0.4115, -0.3680]])\n",
      "name: alpha_w, param: 0.156248539686203\n",
      "name: beta_w, param: -0.1287534236907959\n",
      "name: alpha_x, param: 0.3171935975551605\n",
      "name: beta_x, param: 0.419644832611084\n",
      "name: module.weight, param: tensor([[-0.0037,  0.2682, -0.4115, -0.3680]])\n",
      "name: module.bias, param: tensor([-0.1926])\n",
      "--------after--------\n",
      "y: tensor([[-5150.3291]], grad_fn=<AddmmBackward0>)\n",
      "name: full_precision_weight, param: tensor([[-10000.0039,  -9999.7314, -10000.4111, -10000.3682]])\n",
      "name: alpha_w, param: 0.156248539686203\n",
      "name: beta_w, param: -0.1287534236907959\n",
      "name: alpha_x, param: 0.3171935975551605\n",
      "name: beta_x, param: 0.419644832611084\n",
      "name: module.weight, param: tensor([[-10000.0039,  -9999.7314, -10000.4111, -10000.3682]])\n",
      "name: module.bias, param: tensor([-1.1926])\n",
      "name: full_precision_weight, param: tensor([[10000., 10000., 10000., 10000.]])\n",
      "name: alpha_w, param: 0.0\n",
      "name: beta_w, param: 0.0\n",
      "name: alpha_x, param: None\n",
      "name: beta_x, param: None\n",
      "name: module.weight, param: None\n",
      "name: module.bias, param: tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "init_seeds(0)\n",
    "\n",
    "model = nn.Linear(4, 1)\n",
    "qt_model = QuantizedWrapper(model, quant_w=True, quant_x=True, lsq=2, \n",
    "                            bitwidth_w=4, intbit_w=0, bitwidth_x=4, intbit_x=0, alpha_init=2)\n",
    "optimizer = optim.SGD(qt_model.parameters(), lr = 1)\n",
    "\n",
    "print('--------before--------')\n",
    "for name, param in qt_model.named_parameters():\n",
    "    print(f'name: {name}, param: {param.data}')\n",
    "\n",
    "qt_model.zero_grad()\n",
    "x = torch.tensor((10000,10000,10000,10000)).view(1, 4)\n",
    "y = qt_model(x)\n",
    "y.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print('--------after--------')\n",
    "print(f'y: {y}')\n",
    "for name, param in qt_model.named_parameters():\n",
    "    print(f'name: {name}, param: {param.data}')\n",
    "\n",
    "for name, param in qt_model.named_parameters():\n",
    "    print(f'name: {name}, param: {param.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xv, alpha_wv, beta_wv =  0.2682, 0.156248539686203, -0.1287534236907959"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "tensor([0.3400], grad_fn=<AddBackward0>)\n",
      "tensor([1.])\n",
      "tensor([0.4595])\n",
      "tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([xv], requires_grad=True)\n",
    "alpha_w = torch.tensor([alpha_wv], requires_grad=True)\n",
    "beta_w = torch.tensor([beta_wv], requires_grad=True)\n",
    "\n",
    "print(x.grad)\n",
    "print(alpha_w.grad)\n",
    "print(beta_w.grad)\n",
    "y2 = alpha_w*STEWrapper.apply((x-beta_w)/alpha_w).clamp(-8, 7)+beta_w\n",
    "y2.backward()\n",
    "print(y2)\n",
    "print(x.grad)\n",
    "print(alpha_w.grad)\n",
    "print(beta_w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "tensor([0.3400], grad_fn=<LSQuantBackward>)\n",
      "tensor([1.])\n",
      "tensor([0.4595])\n",
      "tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([xv], requires_grad=True)\n",
    "alpha_w = torch.tensor([alpha_wv], requires_grad=True)\n",
    "beta_w = torch.tensor([beta_wv], requires_grad=True)\n",
    "g_w = torch.tensor([1.])\n",
    "\n",
    "print(x.grad)\n",
    "print(alpha_w.grad)\n",
    "print(beta_w.grad)\n",
    "y1 = LSQuant.apply(x, alpha_w, beta_w, g_w, -8, 7) \n",
    "y1.backward()\n",
    "print(y1)\n",
    "print(x.grad)\n",
    "print(alpha_w.grad)\n",
    "print(beta_w.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
